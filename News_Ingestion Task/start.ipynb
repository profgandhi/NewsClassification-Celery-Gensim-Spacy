{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: application that collects news articles from various RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeds\n",
    "links = [\n",
    "    \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "    \"http://qz.com/feed\",\n",
    "    \"http://feeds.foxnews.com/foxnews/politics\",\n",
    "    \"http://feeds.reuters.com/reuters/businessNews\",\n",
    "    \"http://feeds.feedburner.com/NewshourWorld\",\n",
    "    \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nParse each feed and extract relevant information from each news article,\\nincluding title, content, publication date, and source URL.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed Parser\n",
    "'''\n",
    "Parse each feed and extract relevant information from each news article,\n",
    "including title, content, publication date, and source URL.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_parser(link):\n",
    "    r = requests.get(link) \n",
    "    soup = BeautifulSoup(r.content, 'xml')\n",
    "    items = soup.find_all('item')\n",
    "\n",
    "    a = pd.DataFrame(columns=['title','link','pub_date','description','content'])\n",
    "    for item in items:\n",
    "        title = item.title.text\n",
    "        link = item.link.text\n",
    "        try:\n",
    "            pub_date = item.pubDate.text\n",
    "        except:\n",
    "            pub_date = None\n",
    "        try:\n",
    "            description = item.description.text\n",
    "        except:\n",
    "            description = None\n",
    "        try:\n",
    "            content = item.description.text\n",
    "        except:\n",
    "            content = None\n",
    "\n",
    "        a.loc[len(a)] = [title,link,pub_date,description,content]\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(links):\n",
    "    #df = pd.DataFrame(columns=['title','link','pub_date','description'])\n",
    "    p = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            a = item_parser(link)\n",
    "            p.append(a)\n",
    "            #df=pd.concat([df,a],axis=0)\n",
    "        except:\n",
    "            print(link)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://feeds.reuters.com/reuters/businessNews\n"
     ]
    }
   ],
   "source": [
    "n= get_data(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some on-air claims about Dominion Voting Syste...</td>\n",
       "      <td>https://www.cnn.com/business/live-news/fox-new...</td>\n",
       "      <td>Wed, 19 Apr 2023 12:44:51 GMT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dominion still has pending lawsuits against el...</td>\n",
       "      <td>https://www.cnn.com/business/live-news/fox-new...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here are the 20 specific Fox broadcasts and tw...</td>\n",
       "      <td>https://www.cnn.com/2023/04/17/media/dominion-...</td>\n",
       "      <td>Mon, 17 Apr 2023 16:01:11 GMT</td>\n",
       "      <td>• Fox-Dominion trial delay 'is not unusual,' j...</td>\n",
       "      <td>• Fox-Dominion trial delay 'is not unusual,' j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judge in Fox News-Dominion defamation trial: '...</td>\n",
       "      <td>https://www.cnn.com/2023/04/18/media/fox-domin...</td>\n",
       "      <td>Wed, 19 Apr 2023 08:28:17 GMT</td>\n",
       "      <td>The judge just announced in court that a settl...</td>\n",
       "      <td>The judge just announced in court that a settl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Difficult to say with a straight face': Tappe...</td>\n",
       "      <td>https://www.cnn.com/videos/politics/2023/04/18...</td>\n",
       "      <td>Tue, 18 Apr 2023 21:17:44 GMT</td>\n",
       "      <td>A settlement has been reached in Dominion Voti...</td>\n",
       "      <td>A settlement has been reached in Dominion Voti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indian PM Modi inaugurates temple in Ayodhya</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-68...</td>\n",
       "      <td>Mon, 22 Jan 2024 08:06:54 GMT</td>\n",
       "      <td>The opening of the grand temple comes ahead of...</td>\n",
       "      <td>The opening of the grand temple comes ahead of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Plane gets jammed under bridge in India</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-67...</td>\n",
       "      <td>Fri, 29 Dec 2023 23:17:00 GMT</td>\n",
       "      <td>The scrap fuselage was on its way to Assam fro...</td>\n",
       "      <td>The scrap fuselage was on its way to Assam fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Intruder jumps on table in Indian parliament</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-67...</td>\n",
       "      <td>Wed, 13 Dec 2023 09:49:30 GMT</td>\n",
       "      <td>Lawmakers said two men jumped into the well of...</td>\n",
       "      <td>Lawmakers said two men jumped into the well of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Moment worker is pulled from collapsed tunnel</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-67...</td>\n",
       "      <td>Tue, 28 Nov 2023 15:13:21 GMT</td>\n",
       "      <td>Fourty-one Indian workers are finally rescued ...</td>\n",
       "      <td>Fourty-one Indian workers are finally rescued ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>First video of trapped tunnel workers in India</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-india-67...</td>\n",
       "      <td>Tue, 21 Nov 2023 05:58:33 GMT</td>\n",
       "      <td>Rescuers have shared a video of the 41 workers...</td>\n",
       "      <td>Rescuers have shared a video of the 41 workers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Some on-air claims about Dominion Voting Syste...   \n",
       "1   Dominion still has pending lawsuits against el...   \n",
       "2   Here are the 20 specific Fox broadcasts and tw...   \n",
       "3   Judge in Fox News-Dominion defamation trial: '...   \n",
       "4   'Difficult to say with a straight face': Tappe...   \n",
       "..                                                ...   \n",
       "15       Indian PM Modi inaugurates temple in Ayodhya   \n",
       "16            Plane gets jammed under bridge in India   \n",
       "17       Intruder jumps on table in Indian parliament   \n",
       "18      Moment worker is pulled from collapsed tunnel   \n",
       "19     First video of trapped tunnel workers in India   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://www.cnn.com/business/live-news/fox-new...   \n",
       "1   https://www.cnn.com/business/live-news/fox-new...   \n",
       "2   https://www.cnn.com/2023/04/17/media/dominion-...   \n",
       "3   https://www.cnn.com/2023/04/18/media/fox-domin...   \n",
       "4   https://www.cnn.com/videos/politics/2023/04/18...   \n",
       "..                                                ...   \n",
       "15  https://www.bbc.co.uk/news/world-asia-india-68...   \n",
       "16  https://www.bbc.co.uk/news/world-asia-india-67...   \n",
       "17  https://www.bbc.co.uk/news/world-asia-india-67...   \n",
       "18  https://www.bbc.co.uk/news/world-asia-india-67...   \n",
       "19  https://www.bbc.co.uk/news/world-asia-india-67...   \n",
       "\n",
       "                         pub_date  \\\n",
       "0   Wed, 19 Apr 2023 12:44:51 GMT   \n",
       "1                            None   \n",
       "2   Mon, 17 Apr 2023 16:01:11 GMT   \n",
       "3   Wed, 19 Apr 2023 08:28:17 GMT   \n",
       "4   Tue, 18 Apr 2023 21:17:44 GMT   \n",
       "..                            ...   \n",
       "15  Mon, 22 Jan 2024 08:06:54 GMT   \n",
       "16  Fri, 29 Dec 2023 23:17:00 GMT   \n",
       "17  Wed, 13 Dec 2023 09:49:30 GMT   \n",
       "18  Tue, 28 Nov 2023 15:13:21 GMT   \n",
       "19  Tue, 21 Nov 2023 05:58:33 GMT   \n",
       "\n",
       "                                          description  \\\n",
       "0                                                None   \n",
       "1                                                None   \n",
       "2   • Fox-Dominion trial delay 'is not unusual,' j...   \n",
       "3   The judge just announced in court that a settl...   \n",
       "4   A settlement has been reached in Dominion Voti...   \n",
       "..                                                ...   \n",
       "15  The opening of the grand temple comes ahead of...   \n",
       "16  The scrap fuselage was on its way to Assam fro...   \n",
       "17  Lawmakers said two men jumped into the well of...   \n",
       "18  Fourty-one Indian workers are finally rescued ...   \n",
       "19  Rescuers have shared a video of the 41 workers...   \n",
       "\n",
       "                                              content  \n",
       "0                                                None  \n",
       "1                                                None  \n",
       "2   • Fox-Dominion trial delay 'is not unusual,' j...  \n",
       "3   The judge just announced in court that a settl...  \n",
       "4   A settlement has been reached in Dominion Voti...  \n",
       "..                                                ...  \n",
       "15  The opening of the grand temple comes ahead of...  \n",
       "16  The scrap fuselage was on its way to Assam fro...  \n",
       "17  Lawmakers said two men jumped into the well of...  \n",
       "18  Fourty-one Indian workers are finally rescued ...  \n",
       "19  Rescuers have shared a video of the 41 workers...  \n",
       "\n",
       "[173 rows x 5 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(n,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://feeds.reuters.com/reuters/businessNews\n"
     ]
    }
   ],
   "source": [
    "df = get_data(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from config import postgresql as settings\n",
    "# Example using SQLAlchemy for database storage\n",
    "from sqlalchemy import create_engine, Column, String, DateTime, MetaData\n",
    "from sqlalchemy.orm import sessionmaker,declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine(user, passwd, host, port, db):\n",
    "    url = f\"postgresql://{user}:{passwd}@{host}:{port}/{db}\"\n",
    "    if not database_exists(url):\n",
    "        create_database(url)\n",
    "    engine = create_engine(url, pool_size=50, echo=False)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class NewsArticle(Base):\n",
    "    __tablename__ = 'news_articles'\n",
    "\n",
    "    link = Column(String, primary_key=True)\n",
    "    title = Column(String)\n",
    "    description = Column(String)\n",
    "    content = Column(String)\n",
    "    pub_date = Column(DateTime)\n",
    "    \n",
    "\n",
    "engine = get_engine(settings['pguser'],\n",
    "                    settings['pgpasswd'],\n",
    "                    settings['pghost'],\n",
    "                    settings['pgport'],\n",
    "                    settings['pgdb'])\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "def store_articles_in_database(articles: pd.DataFrame):\n",
    "    for i,article in articles.iterrows():\n",
    "        # Check for duplicates before adding to the database\n",
    "        existing_article = session.query(NewsArticle).filter_by(title=article['link']).first()\n",
    "        if not existing_article:\n",
    "            db_article = NewsArticle(**article.to_dict())\n",
    "            session.add(db_article)\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_articles_in_database(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "from nltk import classify_news_category  # Implement your own classification function\n",
    "\n",
    "app = Celery('news_processing')\n",
    "app.config_from_object('celery_config')\n",
    "\n",
    "@app.task\n",
    "def process_news_articles(articles):\n",
    "    for article in articles:\n",
    "        category = classify_news_category(article['content'])\n",
    "        article['category'] = category\n",
    "        update_database_with_category(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Categories the news item should fall under are:\n",
    "● Terrorism / protest / political unrest / riot\n",
    "● Positive/Uplifting\n",
    "● Natural Disasters\n",
    "● Others\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save('vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load('vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19816 ,  0.41016 , -0.11844 , -0.71497 , -0.74505 , -0.3282  ,\n",
       "        0.023166, -0.64659 ,  0.19508 , -1.213   ,  0.78447 , -1.493   ,\n",
       "       -2.393   ,  0.89226 ,  0.9995  , -0.12802 , -0.33537 , -0.12927 ,\n",
       "        0.40244 , -1.1615  , -0.027187, -0.68055 , -0.59593 ,  0.1749  ,\n",
       "        0.23277 ,  0.96935 , -0.80234 ,  1.6536  , -0.74963 ,  0.66573 ,\n",
       "       -0.26484 , -0.042629,  0.96103 ,  0.71919 ,  0.2738  , -0.22882 ,\n",
       "       -1.0678  ,  0.72851 ,  0.13782 ,  0.32018 ,  0.64071 , -0.62929 ,\n",
       "       -0.23395 ,  0.044309, -0.76914 ,  0.60806 , -0.38999 ,  0.081423,\n",
       "        0.17033 ,  1.3904  ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv[\"floods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'’m', 'become', 'several', 'her', \"'d\", 'thereupon', 'last', 'the', 'noone', 'two', 'ourselves', 'another', 'as', 'afterwards', 'please', 'one', 'against', 'these', '‘s', 'thence', 'because', 'forty', 'sometimes', 'perhaps', 'now', 'do', 'anywhere', 'we', 'how', 'ca', 'during', 'below', 'further', 'amongst', 'had', 'first', 'themselves', 'about', 'this', 'every', 'everything', 'whereas', 'must', 'across', 'have', 'beyond', 'by', 'made', 'therefore', 'empty', 'ten', 'those', 'once', 'whole', 'nowhere', 'out', 'third', \"'ll\", 'n‘t', \"'ve\", 'name', 'between', 'front', 'whence', 'ever', 'various', 'off', 'nothing', 'should', 'upon', 'done', 'whom', 'besides', 'doing', '‘m', 'if', 'your', 'least', 'whereupon', '’ve', 'formerly', 'give', 'it', 'serious', 'up', 'six', 'were', 'around', 'hence', 'seems', 'both', 'regarding', 'its', 'myself', 'eight', 'so', 'there', 'himself', 'for', 'am', 'still', 'call', 'can', 'yet', 'without', 'mostly', 'does', 'alone', 'being', 'side', 'thereafter', 'elsewhere', '‘ll', 'nobody', 'into', 'he', 'full', 'back', 'fifteen', 'make', 'nine', 'namely', 'meanwhile', 'no', 'already', 'very', 'toward', 'since', 'such', 'wherein', 'sixty', 'in', 'everywhere', 'also', 'here', 'put', 'just', 'seeming', 'is', 'through', 'using', 'their', 'many', 'are', 'and', 'yourselves', 'most', 'even', 'unless', 'would', 'all', 'his', 'sometime', 'fifty', 'before', '‘re', 'latter', 'than', 'onto', 'herein', 'of', 'until', 'twenty', 'move', 'that', 'ours', 'neither', 'to', 'which', 'again', 'yourself', \"n't\", 'hers', 'otherwise', 'yours', 'thru', 'eleven', 're', 'she', 'same', 'twelve', 'hereby', 'though', 'top', \"'re\", 'hereupon', 'seem', 'other', 'never', 'did', 'always', 'others', 'wherever', '’re', 'something', 'indeed', 'whoever', 'beside', 'three', 'n’t', 'from', 'well', 'few', 'me', 'moreover', 'part', 'could', 'after', 'only', 'amount', 'with', 'an', 'go', 'not', 'anyway', 'behind', 'whatever', 'my', \"'s\", 'who', 'any', 'beforehand', 'may', 'nevertheless', 'each', 'none', 'often', 'under', 'due', 'you', 'became', 'him', 'becoming', 'someone', 'where', 'almost', 'over', 'therein', 'whenever', 'whereby', 'anyhow', '‘d', 'more', 'via', 'or', 'used', 'whose', 'down', 'whether', 'but', 'some', 'latterly', 'why', \"'m\", 'thereby', 'really', 'on', '’d', 'anyone', 'while', 'except', 'they', '’ll', 'i', 'too', 'else', 'five', 'quite', 'per', 'enough', 'own', 'get', 'however', 'what', 'itself', 'less', 'everyone', 'keep', 'much', 'either', 'whereafter', 'us', 'together', 'hereafter', 'throughout', 'nor', 'above', 'when', '’s', 'a', 'becomes', 'towards', 'might', 'will', 'bottom', 'rather', 'show', 'among', 'somehow', 'say', 'within', 'our', 'seemed', 'somewhere', 'anything', 'whither', 'then', 'has', 'hundred', 'herself', 'be', 'was', 'four', 'take', '‘ve', 'former', 'although', 'cannot', 'next', 'along', 'at', 'them', 'mine', 'see', 'thus', 'been'}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "\n",
    "    # print(mytokens)\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.lower()\n",
    "df['content'] = df['content'].apply(remove_html)\n",
    "df['tokens'] = df['content'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vec'] = df['tokens'].apply(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    [\"terrorism\", \"protest\"],\n",
    "    [\"positive\",\"uplifting\"],\n",
    "    [\"earthquake\", \"floods\"] , # natural disasters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Political_Unreset','Positive/Uplifting','Natural_Disaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array([sent_vec(i) for i in classes])\n",
    "def get_class(vector):\n",
    "    similarity = KeyedVectors.cosine_similarities(vector, vectors)\n",
    "    if(max(similarity)) >= 0.65:\n",
    "        return class_names[np.argmax(similarity)]\n",
    "    return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['vec'].apply(get_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                 92\n",
       "Political_Unreset     61\n",
       "Positive/Uplifting     1\n",
       "Natural_Disaster       1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
